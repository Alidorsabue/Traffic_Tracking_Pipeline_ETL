from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import sys
import pandas as pd
sys.path.insert(0, '/opt/airflow')

from src.Script_ETL import (
    extract_recent_data, clean_data, detect_congestion, 
    aggregate_by_zone, load_to_db, aggregate_by_edge, load_edge_agg_to_db, load_predictions_to_db,
    load_mapmatching_from_cache
)
# Note: Les imports de mapmatching et model sont d√©plac√©s dans les fonctions pour √©viter
# les timeouts lors du chargement du DAG (osmnx est tr√®s lourd √† importer)

default_args = {
    'owner': 'congestion_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

# Args sp√©cifiques pour mapmatching (maintenant rapide car utilise le cache)
# OPTIMISATION: Plus besoin de timeout strict car on charge depuis la base de donn√©es
mapmatching_args = {
    'owner': 'congestion_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'execution_timeout': timedelta(minutes=2),  # Timeout court car c'est juste une lecture DB
    'email_on_failure': False,
    'email_on_retry': False,
}

def extract_task(**context):
    """Extrait les donn√©es GPS r√©centes de la base de donn√©es."""
    try:
        print("üîç Extraction des donn√©es GPS r√©centes...")
        df = extract_recent_data()
        
        if df is None:
            print("‚ùå ERREUR: extract_recent_data() a retourn√© None")
            return pd.DataFrame()
        
        if isinstance(df, pd.DataFrame):
            if df.empty:
                print("‚ö†Ô∏è ATTENTION: Aucune donn√©e GPS trouv√©e dans la table gps_points")
                print("   V√©rifier que l'app mobile envoie des donn√©es √† la base de donn√©es")
                return pd.DataFrame()
            else:
                print(f"‚úÖ {len(df)} lignes GPS extraites")
                print(f"   P√©riode: {df['timestamp'].min()} √† {df['timestamp'].max()}")
                return df
        else:
            print(f"‚ùå ERREUR: extract_recent_data() a retourn√© un type inattendu: {type(df)}")
            return pd.DataFrame()
    except Exception as e:
        print(f"‚ùå ERREUR lors de l'extraction: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def clean_task(**context):
    """Nettoie les donn√©es GPS (suppression des doublons, filtrage des vitesses aberrantes)."""
    ti = context['ti']
    df = ti.xcom_pull(task_ids='extract_data')
    if df is None or df.empty:
        return pd.DataFrame()
    if isinstance(df, pd.DataFrame):
        df_clean = clean_data(df)
        return df_clean
    return pd.DataFrame()

def mapmatching_task(**context):
    """
    Charge les r√©sultats de map matching depuis le cache (mapmatching_cache).
    OPTIMISATION: Utilise le cache au lieu d'ex√©cuter mapmatching √† chaque fois.
    Le map matching est effectu√© une fois par heure par le DAG mapmatching_cache_hourly.
    Cette t√¢che est maintenant ultra-rapide car elle charge simplement depuis la base de donn√©es.
    """
    try:
        print("üì¶ Chargement des r√©sultats de map matching depuis le cache...")
        
        # Charger depuis le cache (derni√®re heure)
        df_matched = load_mapmatching_from_cache(hours_back=1)
        
        if df_matched.empty:
            print("‚ö†Ô∏è Aucune donn√©e dans le cache mapmatching. Le DAG mapmatching_cache_hourly doit s'ex√©cuter pour remplir le cache.")
            print("   Note: Le pipeline continuera mais sans donn√©es match√©es pour l'agr√©gation par edge.")
            return pd.DataFrame()
        
        # V√©rifier que les colonnes n√©cessaires sont pr√©sentes
        required_cols = ['edge_u', 'edge_v', 'speed', 'timestamp']
        missing_cols = set(required_cols) - set(df_matched.columns)
        if missing_cols:
            print(f"‚ö†Ô∏è Colonnes manquantes dans le cache: {missing_cols}")
            return pd.DataFrame()
        
        # Compter les points match√©s
        matched_count = df_matched['edge_u'].notna().sum() if 'edge_u' in df_matched.columns else 0
        print(f"‚úÖ {len(df_matched)} points charg√©s depuis le cache: {matched_count} match√©s ({matched_count/len(df_matched)*100:.1f}%)")
        
        return df_matched
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du chargement depuis le cache: {e}")
        import traceback
        traceback.print_exc()
        # Retourner un DataFrame vide plut√¥t que de faire √©chouer la t√¢che
        # Cela permet aux autres branches du pipeline de continuer
        # Les t√¢ches suivantes avec trigger_rule='all_done' s'ex√©cuteront quand m√™me
        return pd.DataFrame()

def detect_congestion_task(**context):
    """D√©tecte les embouteillages bas√©s sur la vitesse et l'heure."""
    ti = context['ti']
    df_clean = ti.xcom_pull(task_ids='clean_data')
    if df_clean is None or (isinstance(df_clean, pd.DataFrame) and df_clean.empty):
        return pd.DataFrame()
    df_congestion = detect_congestion(df_clean)
    return df_congestion

def aggregate_by_zone_task(**context):
    """Agr√®ge les donn√©es de congestion par zones g√©ographiques."""
    ti = context['ti']
    df_congestion = ti.xcom_pull(task_ids='detect_congestion')
    if df_congestion is None or (isinstance(df_congestion, pd.DataFrame) and df_congestion.empty):
        return pd.DataFrame()
    df_aggregated = aggregate_by_zone(df_congestion)
    return df_aggregated

def load_congestion_task(**context):
    """Charge les donn√©es agr√©g√©es de congestion dans la table congestion."""
    ti = context['ti']
    df_aggregated = ti.xcom_pull(task_ids='aggregate_data')
    
    if df_aggregated is None:
        print("load_congestion_task: Aucune donn√©e XCom re√ßue de aggregate_data")
        return "no_data"
    
    if isinstance(df_aggregated, pd.DataFrame) and df_aggregated.empty:
        print("load_congestion_task: DataFrame congestion vide, aucune donn√©e √† charger")
        return "empty_data"
    
    if not isinstance(df_aggregated, pd.DataFrame):
        print(f"load_congestion_task: Type de donn√©es inattendu: {type(df_aggregated)}")
        return "invalid_data"
    
    # Charger les donn√©es
    success = load_to_db(df_aggregated)
    
    if success:
        print(f"load_congestion_task: Succ√®s - {len(df_aggregated)} lignes charg√©es")
        return "success"
    else:
        print(f"load_congestion_task: √âchec du chargement")
        return "error"

def aggregate_by_edge_task(**context):
    """
    Agr√®ge les donn√©es GPS par tron√ßon de route (edge) pour l'analyse pr√©dictive.
    Continue m√™me si mapmatching a √©chou√© (retourne DataFrame vide).
    """
    try:
        ti = context['ti']
        df_matched = ti.xcom_pull(task_ids='mapmatching')
        
        if df_matched is None:
            print("Aucune donn√©e match√©e disponible pour l'agr√©gation par edge (XCom None)")
            return pd.DataFrame()
        
        if isinstance(df_matched, pd.DataFrame) and df_matched.empty:
            print("DataFrame match√© vide, pas d'agr√©gation par edge possible")
            return pd.DataFrame()
        
        if isinstance(df_matched, pd.DataFrame):
            # V√©rifier que les colonnes n√©cessaires sont pr√©sentes
            required_cols = ['edge_u', 'edge_v', 'speed', 'timestamp']
            missing_cols = set(required_cols) - set(df_matched.columns)
            if missing_cols:
                print(f"Colonnes manquantes pour l'agr√©gation par edge: {missing_cols}")
                return pd.DataFrame()
            
            df_edge_agg = aggregate_by_edge(df_matched, time_interval_minutes=10)
            if df_edge_agg is not None and not df_edge_agg.empty:
                print(f"Agr√©gation par edge termin√©e: {len(df_edge_agg)} tron√ßons agr√©g√©s")
            else:
                print("R√©sultat de l'agr√©gation par edge vide")
            return df_edge_agg if df_edge_agg is not None else pd.DataFrame()
        
        print(f"Type de donn√©es inattendu pour mapmatching: {type(df_matched)}")
        return pd.DataFrame()
    except Exception as e:
        print(f"Erreur lors de l'agr√©gation par edge: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def load_edge_agg_task(**context):
    """Charge les donn√©es agr√©g√©es par edge dans la table edge_agg."""
    ti = context['ti']
    df_edge_agg = ti.xcom_pull(task_ids='aggregate_edge')
    
    if df_edge_agg is None:
        print("load_edge_agg_task: Aucune donn√©e XCom re√ßue de aggregate_edge")
        return "no_data"
    
    if isinstance(df_edge_agg, pd.DataFrame) and df_edge_agg.empty:
        print("load_edge_agg_task: DataFrame edge_agg vide, aucune donn√©e √† charger")
        return "empty_data"
    
    if not isinstance(df_edge_agg, pd.DataFrame):
        print(f"load_edge_agg_task: Type de donn√©es inattendu: {type(df_edge_agg)}")
        return "invalid_data"
    
    # Charger les donn√©es
    success = load_edge_agg_to_db(df_edge_agg)
    
    if success:
        print(f"load_edge_agg_task: Succ√®s - {len(df_edge_agg)} lignes charg√©es")
        return "success"
    else:
        print(f"load_edge_agg_task: √âchec du chargement")
        return "error"

def train_model_task(**context):
    """
    Entra√Æne le mod√®le Random Forest pour pr√©dire la vitesse future.
    Cette t√¢che peut √™tre ex√©cut√©e moins fr√©quemment (ex: une fois par jour).
    """
    # Import lazy pour √©viter le timeout au chargement du DAG
    from src.model import train_model
    
    model = train_model()
    if model is not None:
        print("Mod√®le entra√Æn√© avec succ√®s")
        return "success"
    else:
        print("√âchec de l'entra√Ænement du mod√®le")
        return "failed"

def predict_task(**context):
    """
    Pr√©dit la vitesse future pour les tron√ßons r√©cents.
    Utilise le mod√®le pr√©-entra√Æn√© pour g√©n√©rer des pr√©dictions.
    """
    try:
        # Import lazy pour √©viter le timeout au chargement du DAG
        from src.model import predict_next
        
        df_predictions = predict_next()
        if df_predictions is None or (isinstance(df_predictions, pd.DataFrame) and df_predictions.empty):
            print("Aucune pr√©diction g√©n√©r√©e")
            return pd.DataFrame()
        print(f"Pr√©dictions g√©n√©r√©es: {len(df_predictions)} lignes")
        return df_predictions
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration des pr√©dictions: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def load_predictions_task(**context):
    """Charge les pr√©dictions dans la table predictions."""
    ti = context['ti']
    df_predictions = ti.xcom_pull(task_ids='predict')
    
    if df_predictions is None:
        print("load_predictions_task: Aucune donn√©e XCom re√ßue de predict")
        return "no_data"
    
    if isinstance(df_predictions, pd.DataFrame) and df_predictions.empty:
        print("load_predictions_task: DataFrame predictions vide, aucune pr√©diction √† charger")
        return "empty_data"
    
    if not isinstance(df_predictions, pd.DataFrame):
        print(f"load_predictions_task: Type de donn√©es inattendu: {type(df_predictions)}")
        return "invalid_data"
    
    # Charger les pr√©dictions
    success = load_predictions_to_db(df_predictions)
    
    if success:
        print(f"load_predictions_task: Succ√®s - {len(df_predictions)} pr√©dictions charg√©es")
        return "success"
    else:
        print(f"load_predictions_task: √âchec du chargement")
        return "error"

# DAG 1: D√©tection de congestion rapide (toutes les 10 minutes) - SANS mapmatching
# Ce DAG est l√©ger et rapide, utilise uniquement les coordonn√©es GPS brutes

with DAG(
    'congestion_zone_detection',
    default_args=default_args,
    description='D√©tection rapide de congestion par zones g√©ographiques (sans mapmatching)',
    schedule_interval='*/10 * * * *',  # Ex√©cution toutes les 10 minutes
    catchup=False,
    tags=['congestion', 'traffic', 'quick']
) as dag_congestion:

    extract = PythonOperator(
        task_id='extract_data',
        python_callable=extract_task,
    )

    clean = PythonOperator(
        task_id='clean_data',
        python_callable=clean_task,
    )

    detect = PythonOperator(
        task_id='detect_congestion',
        python_callable=detect_congestion_task,
    )

    aggregate_zone = PythonOperator(
        task_id='aggregate_data',
        python_callable=aggregate_by_zone_task,
    )

    load_congestion = PythonOperator(
        task_id='load_data',
        python_callable=load_congestion_task,
    )

    extract >> clean >> detect >> aggregate_zone >> load_congestion

# DAG 2: Mapmatching et analyse avanc√©e (toutes les 30 minutes) - AVEC mapmatching
# Ce DAG est plus lourd, n√©cessite mapmatching pour l'agr√©gation par edge, ML et alertes

with DAG(
    'traffic_advanced_analysis',
    default_args=default_args,
    description='Analyse avanc√©e du trafic avec mapmatching, ML et alertes',
    schedule_interval='*/30 * * * *',  # Ex√©cution toutes les 30 minutes
    catchup=False,
    tags=['traffic', 'prediction', 'mapmatching', 'alerts']
) as dag_advanced:

    extract_advanced = PythonOperator(
        task_id='extract_data',
        python_callable=extract_task,
    )

    clean_advanced = PythonOperator(
        task_id='clean_data',
        python_callable=clean_task,
    )

    mapmatching = PythonOperator(
        task_id='mapmatching',
        python_callable=mapmatching_task,
        **mapmatching_args,
        # OPTIMISATION: Permettre √† la t√¢che suivante de continuer m√™me si mapmatching timeout
        # Retourner un DataFrame vide plut√¥t que d'√©chouer pour ne pas bloquer
        do_xcom_push=True,  # Push le r√©sultat m√™me en cas d'erreur
    )

    aggregate_edge = PythonOperator(
        task_id='aggregate_edge',
        python_callable=aggregate_by_edge_task,
    )

    load_edge_agg = PythonOperator(
        task_id='load_edge_agg',
        python_callable=load_edge_agg_task,
    )

    train_ml = PythonOperator(
        task_id='train_model',
        python_callable=train_model_task,
    )

    predict = PythonOperator(
        task_id='predict',
        python_callable=predict_task,
    )

    load_predictions = PythonOperator(
        task_id='load_predictions',
        python_callable=load_predictions_task,
    )

    # OPTIMISATION: Permettre aux t√¢ches suivantes de s'ex√©cuter m√™me si mapmatching √©choue ou timeout
    # Utiliser 'all_done' pour que aggregate_edge s'ex√©cute m√™me si mapmatching √©choue
    extract_advanced >> clean_advanced >> mapmatching >> aggregate_edge >> load_edge_agg
    
    # ML et pr√©dictions s'ex√©cutent apr√®s edge_agg
    load_edge_agg >> train_ml >> predict >> load_predictions
    
    # Note: Les alertes proactives sont maintenant g√©r√©es par le DAG s√©par√© 'proactive_alerts'
    # qui s'ex√©cute toutes les 10 minutes pour une meilleure r√©activit√©
    # Le DAG proactive_alerts ne n√©cessite pas mapmatching, il utilise directement edge_agg
    
    # Rendre les t√¢ches tol√©rantes aux √©checs - elles s'ex√©cutent m√™me si mapmatching √©choue
    # 'all_done' = s'ex√©cute m√™me si les t√¢ches pr√©c√©dentes ont √©chou√© ou timeout
    aggregate_edge.trigger_rule = 'all_done'
    load_edge_agg.trigger_rule = 'all_done'
    train_ml.trigger_rule = 'all_done'
    predict.trigger_rule = 'all_done'
    load_predictions.trigger_rule = 'all_done'
    
    # OPTIMISATION: Permettre √† aggregate_edge de s'ex√©cuter m√™me si mapmatching timeout
    # Si mapmatching √©choue ou timeout, aggregate_edge recevra un DataFrame vide
    # et retournera un DataFrame vide sans bloquer le reste du pipeline
    load_predictions.trigger_rule = 'all_done'