# dags/Mapmatching_DAG.py
"""
DAG pour effectuer le map matching des donn√©es GPS r√©centes.
S'ex√©cute une fois par heure et stocke les r√©sultats dans mapmatching_cache pour r√©utilisation.
Cela √©vite de bloquer le pipeline principal avec le map matching √† chaque ex√©cution.
"""

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import sys
sys.path.insert(0, '/opt/airflow')

default_args = {
    'owner': 'congestion_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
    'email_on_failure': False,
    'email_on_retry': False,
}

def mapmatching_cache_task(**context):
    """
    Effectue le map matching sur les donn√©es GPS r√©centes et stocke les r√©sultats dans mapmatching_cache.
    Cette t√¢che s'ex√©cute toutes les heures pour maintenir un cache √† jour.
    """
    try:
        # Import lazy pour √©viter le timeout au chargement du DAG
        from src.mapmatching import effectuer_mapmatching
        from src.Script_ETL import extract_recent_data, clean_data, get_db_connection
        import pandas as pd
        
        print("üîÑ D√©but du map matching pour le cache...")
        
        # Extraire les donn√©es r√©centes (derni√®re heure)
        print("üì• Extraction des donn√©es GPS r√©centes (derni√®re heure)...")
        df = extract_recent_data()
        
        if df.empty:
            print("‚ö†Ô∏è Aucune donn√©e GPS r√©cente √† traiter")
            return "no_data"
        
        print(f"‚úÖ {len(df)} points GPS extraits")
        
        # Nettoyer les donn√©es
        print("üßπ Nettoyage des donn√©es...")
        df_clean = clean_data(df)
        
        if df_clean.empty:
            print("‚ö†Ô∏è Aucune donn√©e valide apr√®s nettoyage")
            return "no_valid_data"
        
        print(f"‚úÖ {len(df_clean)} points GPS valides apr√®s nettoyage")
        
        # OPTIMISATION: Traiter un maximum de points mais avec une limite raisonnable
        # Pour le cache horaire, on peut traiter plus de points qu'en temps r√©el
        max_points_to_process = 100  # Plus que le DAG principal car on a plus de temps
        
        if len(df_clean) > max_points_to_process:
            # Prendre les 100 points les plus r√©cents
            df_limited = df_clean.head(max_points_to_process)
            print(f"‚ö†Ô∏è Mapmatching limit√© √† {max_points_to_process} points (sur {len(df_clean)} disponibles)")
        else:
            df_limited = df_clean
            print(f"D√©but mapmatching sur {len(df_limited)} points")
        
        # Ex√©cuter map matching
        print("üó∫Ô∏è Ex√©cution du map matching...")
        df_matched = effectuer_mapmatching(df_limited, max_points=max_points_to_process, max_distance=50)
        
        if df_matched.empty:
            print("‚ö†Ô∏è Aucun r√©sultat apr√®s map matching")
            return "no_match_result"
        
        # Compter les points match√©s
        matched_count = df_matched['edge_u'].notna().sum() if 'edge_u' in df_matched.columns else 0
        print(f"‚úÖ Map matching termin√©: {matched_count}/{len(df_matched)} points match√©s ({matched_count/len(df_matched)*100:.1f}%)")
        
        # Stocker dans mapmatching_cache
        print("üíæ Stockage des r√©sultats dans mapmatching_cache...")
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # Pr√©parer les donn√©es pour insertion
            # Colonnes n√©cessaires: driver_id, latitude, longitude, speed, timestamp, edge_u, edge_v
            required_cols = ['driver_id', 'latitude', 'longitude', 'speed', 'timestamp']
            optional_cols = ['edge_u', 'edge_v', 'osmid', 'road_name', 'distance_to_road']
            
            # V√©rifier que les colonnes requises existent
            missing_cols = set(required_cols) - set(df_matched.columns)
            if missing_cols:
                print(f"‚ùå Colonnes manquantes: {missing_cols}")
                return "missing_columns"
            
            # S√©lectionner les colonnes √† ins√©rer
            cols_to_insert = required_cols.copy()
            for col in optional_cols:
                if col in df_matched.columns:
                    cols_to_insert.append(col)
            
            df_to_insert = df_matched[cols_to_insert].copy()
            
            # Ajouter une colonne processed_at pour marquer quand ces donn√©es ont √©t√© trait√©es
            df_to_insert['processed_at'] = datetime.now()
            
            # Ins√©rer les donn√©es (remplacer les anciennes donn√©es pour la m√™me p√©riode)
            # On supprime d'abord les donn√©es de la derni√®re heure pour √©viter les doublons
            delete_query = """
                DELETE FROM mapmatching_cache 
                WHERE processed_at > NOW() - INTERVAL '2 hours'
            """
            cursor.execute(delete_query)
            deleted_count = cursor.rowcount
            print(f"üóëÔ∏è {deleted_count} anciennes entr√©es supprim√©es du cache")
            
            # Ins√©rer les nouvelles donn√©es
            insert_query = """
                INSERT INTO mapmatching_cache 
                (driver_id, latitude, longitude, speed, timestamp, edge_u, edge_v, osmid, road_name, distance_to_road, processed_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (driver_id, timestamp) DO UPDATE SET
                    edge_u = EXCLUDED.edge_u,
                    edge_v = EXCLUDED.edge_v,
                    osmid = EXCLUDED.osmid,
                    road_name = EXCLUDED.road_name,
                    distance_to_road = EXCLUDED.distance_to_road,
                    processed_at = EXCLUDED.processed_at
            """
            
            values = []
            for _, row in df_to_insert.iterrows():
                values.append((
                    str(row['driver_id']),
                    float(row['latitude']),
                    float(row['longitude']),
                    float(row['speed']),
                    row['timestamp'] if pd.notna(row['timestamp']) else datetime.now(),
                    int(row['edge_u']) if pd.notna(row.get('edge_u')) else None,
                    int(row['edge_v']) if pd.notna(row.get('edge_v')) else None,
                    int(row['osmid']) if pd.notna(row.get('osmid')) else None,
                    str(row['road_name']) if pd.notna(row.get('road_name')) else None,
                    float(row['distance_to_road']) if pd.notna(row.get('distance_to_road')) else None,
                    row['processed_at']
                ))
            
            from psycopg2.extras import execute_values
            execute_values(cursor, insert_query, values)
            conn.commit()
            
            print(f"‚úÖ {len(values)} entr√©es ajout√©es au cache mapmatching")
            return "success"
            
        except Exception as e:
            conn.rollback()
            print(f"‚ùå Erreur lors de l'insertion dans mapmatching_cache: {e}")
            import traceback
            traceback.print_exc()
            return "insert_error"
        finally:
            cursor.close()
            conn.close()
            
    except Exception as e:
        print(f"‚ùå Erreur dans mapmatching_cache_task: {e}")
        import traceback
        traceback.print_exc()
        return "error"

with DAG(
    'mapmatching_cache_hourly',
    default_args=default_args,
    description='Effectue le map matching toutes les heures et stocke les r√©sultats dans le cache',
    schedule_interval='0 * * * *',  # Toutes les heures √† minute 0
    catchup=False,
    tags=['mapmatching', 'cache', 'hourly'],
    max_active_runs=1  # Une seule ex√©cution √† la fois
) as dag:

    mapmatching_cache = PythonOperator(
        task_id='mapmatching_cache',
        python_callable=mapmatching_cache_task,
        execution_timeout=timedelta(minutes=30),  # Timeout de 30 minutes pour cette t√¢che
    )

    mapmatching_cache

